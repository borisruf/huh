The speaker describes Large Language Models (LLMs), which are neural networks used in Generative AI to understand and generate human language. These are not capable of original thought but can produce text that seems unique by imitating learned patterns. They go through several training phases where tokenization occurs - the conversion of information into numerical form - making the text into smaller, separate pieces like words or parts of words. This process aids in better text comprehension by the computer as it can zero in on and process these smaller segments, one at a time.